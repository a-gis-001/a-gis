{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfc3b6-f914-4a9b-be00-3feadad722ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "635e853e-2402-430d-b0b2-f7083047de3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/ww5/.cache/huggingface/hub/models--TheBloke--deepseek-coder-6.7B-instruct-GGUF/snapshots/9e221e6b41cb1bf1c5d8f9718e81e3dc781f7557/deepseek-coder-6.7b-instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = deepseek-ai_deepseek-coder-6.7b-instruct\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scale_linear f32              = 4.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,31757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 32013\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32021\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32014\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 243/32256 vs 237/32256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 32256\n",
      "llm_load_print_meta: n_merges         = 31757\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 100000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.25\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = deepseek-ai_deepseek-coder-6.7b-instruct\n",
      "llm_load_print_meta: BOS token        = 32013 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token        = 32021 '<|EOT|>'\n",
      "llm_load_print_meta: PAD token        = 32014 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token         = 126 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3821.77 MiB, ( 3821.83 / 73728.00)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.88 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3821.76 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 100000.0\n",
      "llama_new_context_with_model: freq_scale = 0.25\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/ww5/a-gis/venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 77309.41 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2048.00 MiB, ( 5871.64 / 73728.00)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    17.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   296.02 MiB, ( 6167.66 / 73728.00)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '32014', 'tokenizer.ggml.eos_token_id': '32021', 'tokenizer.ggml.bos_token_id': '32013', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '32', 'llama.context_length': '16384', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '100000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.rope.scale_linear': '4.000000', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'deepseek-ai_deepseek-coder-6.7b-instruct'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List,Any,Dict,Tuple,Optional\n",
    "from llama_cpp import Llama,llama_types,llama_chat_format\n",
    "\n",
    "def _format_add_colon_single(\n",
    "    system_message: str, messages: List[Tuple[str, Optional[str]]], sep: str\n",
    ") -> str:\n",
    "    \"\"\"Format the prompt with the add-colon-single style.\"\"\"\n",
    "    ret = system_message + sep\n",
    "    for role, message in messages:\n",
    "        if message:\n",
    "            ret += role + \": \" + message + sep\n",
    "        else:\n",
    "            ret += role + \":\"\n",
    "    return ret\n",
    "    \n",
    "def _map_roles(\n",
    "    messages: List[llama_types.ChatCompletionRequestMessage], role_map: Dict[str, str]\n",
    ") -> List[Tuple[str, Optional[str]]]:\n",
    "    \"\"\"Map the message roles.\"\"\"\n",
    "    output: List[Tuple[str, Optional[str]]] = []\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        if role in role_map:\n",
    "            output.append((role_map[role], message[\"content\"]))\n",
    "    return output\n",
    "\n",
    "def format_deepseek(\n",
    "    messages: List[llama_types.ChatCompletionRequestMessage],\n",
    "    **kwargs: Any,\n",
    ") -> llama_chat_format.ChatFormatterResponse:\n",
    "    _roles = dict(user=\"### Instruction:\", assistant=\"### Assistant:\")\n",
    "    _sep = \"\\n\"\n",
    "    _system_message = f'''You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.'''\n",
    "    _messages = _map_roles(messages, _roles)\n",
    "    _messages.append((_roles[\"assistant\"], None))\n",
    "    _prompt = _format_add_colon_single(_system_message, _messages, _sep)\n",
    "    return llama_chat_format.ChatFormatterResponse(prompt=_prompt)\n",
    "\n",
    "model = \"/Users/ww5/.cache/huggingface/hub/models--TheBloke--deepseek-coder-6.7B-instruct-GGUF/snapshots/9e221e6b41cb1bf1c5d8f9718e81e3dc781f7557/deepseek-coder-6.7b-instruct.Q4_K_M.gguf\"\n",
    "llm = Llama(\n",
    "      model_path=model,\n",
    "      chat_handler=format_deepseek,\n",
    "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    "\n",
    ")\n",
    "\n",
    "# result=llm.create_chat_completion(\n",
    "#       messages = [\n",
    "#           {\"role\": \"system\", \"content\": \"You are a python code developer. You use the bash command line and git commands to modify the A_GIS package repository according to a request.\"},\n",
    "#           {\n",
    "#               \"role\": \"user\",\n",
    "#               \"content\": \"Implement a function to sample from a uniform distribution.\"\n",
    "#           }\n",
    "#       ]\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fddd9912-0728-4a92-af0b-4c32e4ee68d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatFormatterResponse(prompt='You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\\n### Instruction:: Implement a function to sample from a uniform distribution.\\n### Assistant::', stop=None)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90d83b54-fd51-4f9b-b6e3-ede723cf96f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     292.04 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /    16 runs   (    0.09 ms per token, 10876.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     291.90 ms /    12 tokens (   24.33 ms per token,    41.11 tokens per second)\n",
      "llama_print_timings:        eval time =     263.32 ms /    15 runs   (   17.55 ms per token,    56.96 tokens per second)\n",
      "llama_print_timings:       total time =     579.90 ms /    27 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-854d9c81-10c3-4f25-9756-afde4f43bd35',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709488977,\n",
       " 'model': '/Users/ww5/.cache/huggingface/hub/models--TheBloke--deepseek-coder-6.7B-instruct-GGUF/snapshots/9e221e6b41cb1bf1c5d8f9718e81e3dc781f7557/deepseek-coder-6.7b-instruct.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '\\n\\n\\n```python\\ndef generate_uniform(low, high):\\n',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 12, 'completion_tokens': 16, 'total_tokens': 28}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Implement a function to sample from a uniform distribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e86cb9de-7ef6-450f-bcef-f6ea01e84e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Implement a function to sample from a uniform distribution.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "165f3457-2a56-445f-9719-6e67850402b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatFormatterResponse(prompt='You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\\n### Instruction:: Implement a function to sample from a uniform distribution.\\n### Assistant::', stop=None)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2355397e-9003-4dc8-92ef-a248940bac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     168.92 ms\n",
      "llama_print_timings:      sample time =       1.26 ms /    16 runs   (    0.08 ms per token, 12738.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     692.24 ms /    16 runs   (   43.26 ms per token,    23.11 tokens per second)\n",
      "llama_print_timings:       total time =     713.90 ms /    17 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-d2992b7c-40e6-41b8-a247-3104f0a754db',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709488864,\n",
       " 'model': '/Users/ww5/.cache/huggingface/hub/models--TheBloke--deepseek-coder-6.7B-instruct-GGUF/snapshots/9e221e6b41cb1bf1c5d8f9718e81e3dc781f7557/deepseek-coder-6.7b-instruct.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '\\n<jupyter_code>\\nimport random\\r\\ndef sample',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 12, 'completion_tokens': 16, 'total_tokens': 28}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Implement a function to sample from a uniform distribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ab0a403-495d-40d7-bfc9-0b1d929808dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Write a uniform distribution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19758785-6e68-4738-9148-206f3917971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     292.04 ms\n",
      "llama_print_timings:      sample time =      38.76 ms /   512 runs   (    0.08 ms per token, 13208.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.17 ms /    11 tokens (   24.29 ms per token,    41.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9135.23 ms /   511 runs   (   17.88 ms per token,    55.94 tokens per second)\n",
      "llama_print_timings:       total time =   10081.43 ms /   522 tokens\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "  \"User: {prompt}\\n\\nAssistant:\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc7ab1ae-5668-473e-a182-0fdae0748530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-5f7576ba-36e1-4fb5-927b-3c428ce24fa1', 'object': 'text_completion', 'created': 1709489024, 'model': '/Users/ww5/.cache/huggingface/hub/models--TheBloke--deepseek-coder-6.7B-instruct-GGUF/snapshots/9e221e6b41cb1bf1c5d8f9718e81e3dc781f7557/deepseek-coder-6.7b-instruct.Q4_K_M.gguf', 'choices': [{'text': 'User: {prompt}\\n\\nAssistant: {response}\\nUser: {next_user_input}\\nAssistant: {next_assistant_reply}\\n```\\n\"\"\"\\n\\n\\nclass Conversation:\\n    def __init__(self, max_lines=10):\\n        self.max_lines = max_lines\\n        self.convo = []\\n        self.current = 0\\n        self.template = \"\"\"\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\n\\\\n{line}\\\\', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 12, 'completion_tokens': 512, 'total_tokens': 524}}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9b197-3ccc-4f43-9fa7-3a631c05d7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
